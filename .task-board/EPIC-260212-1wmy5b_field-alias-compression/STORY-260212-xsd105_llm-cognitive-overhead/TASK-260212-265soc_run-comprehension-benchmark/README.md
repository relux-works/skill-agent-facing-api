# TASK-260212-265soc: run-comprehension-benchmark

## Description
Execute the comprehension test against Claude (Haiku and Sonnet for cost efficiency). Run each test set (5/15/30 aliases) x 5 iterations. Score: correct answers / total questions. Compare abbreviated vs full-name control group. Record: accuracy delta, response latency delta, any patterns in error types (does the LLM confuse similar 1-char aliases?). Document findings in .research/260212_alias-comprehension-benchmark.md.

## Scope
(define task scope)

## Acceptance Criteria
(define acceptance criteria)
